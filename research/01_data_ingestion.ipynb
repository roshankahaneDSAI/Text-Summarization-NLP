{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MyCase\\\\Projects\\\\DSAI\\\\portfolio\\\\Text-Summarization-NLP\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\rosha\\AppData\\Local\\Temp\\ipykernel_29132\\1817716843.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  os.chdir(\"D:\\MyCase\\Projects\\DSAI\\portfolio\\Text-Summarization-NLP\")\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"D:\\MyCase\\Projects\\DSAI\\portfolio\\Text-Summarization-NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\MyCase\\\\Projects\\\\DSAI\\\\portfolio\\\\Text-Summarization-NLP'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir \n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from textSummarizer.logging import logger\n",
    "from textSummarizer.utils.common import get_size\n",
    "from urllib import request, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def download_file(self):\n",
    "        dst = Path(self.config.local_data_file)\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        url = self.config.source_URL\n",
    "        resp = requests.get(url, stream=True, allow_redirects=True, timeout=60)\n",
    "        print(\"status:\", resp.status_code)\n",
    "        print(\"headers:\", resp.headers)\n",
    "\n",
    "        resp.raise_for_status()  # raise if 4xx/5xx\n",
    "\n",
    "        with dst.open(\"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        size = dst.stat().st_size\n",
    "        print(\"written bytes:\", size)\n",
    "        if size == 0:\n",
    "            raise RuntimeError(\"Downloaded file is 0 bytes, aborting.\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        zip_path = Path(self.config.local_data_file)\n",
    "        unzip_path = Path(self.config.unzip_dir)\n",
    "        unzip_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(str(zip_path), \"r\") as zip_ref:\n",
    "                logger.info(\"Zip contents: %s\", zip_ref.namelist()[:10])\n",
    "                zip_ref.extractall(str(unzip_path))\n",
    "        except (zipfile.BadZipFile, EOFError) as e:\n",
    "            sample = zip_path.read_bytes()[:1024].decode(errors=\"replace\")\n",
    "            raise RuntimeError(\n",
    "                f\"Zip extraction failed (corrupt/truncated). Size={zip_path.stat().st_size}. Preview:\\n{sample}\"\n",
    "            ) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ...existing code...\n",
    "# import requests\n",
    "# import shutil\n",
    "# import zipfile\n",
    "# from pathlib import Path\n",
    "\n",
    "# class DataIngestion:\n",
    "#     def __init__(self, config: DataIngestionConfig):\n",
    "#         self.config = config\n",
    "\n",
    "#     def download_file(self):\n",
    "#         dst = Path(self.config.local_data_file)\n",
    "#         dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         # atomic download to temporary file then rename\n",
    "#         tmp = dst.with_suffix(\".tmp\")\n",
    "#         url = str(self.config.source_URL)\n",
    "#         logger.info(f\"Downloading from: {url} -> {dst}\")\n",
    "\n",
    "#         try:\n",
    "#             with requests.get(url, stream=True, timeout=120) as r:\n",
    "#                 r.raise_for_status()\n",
    "#                 with tmp.open(\"wb\") as f:\n",
    "#                     for chunk in r.iter_content(chunk_size=8192):\n",
    "#                         if chunk:\n",
    "#                             f.write(chunk)\n",
    "#                     f.flush()\n",
    "#                     os.fsync(f.fileno())\n",
    "#             tmp.replace(dst)  # atomic on same filesystem\n",
    "#         except Exception as e:\n",
    "#             if tmp.exists():\n",
    "#                 tmp.unlink()\n",
    "#             raise RuntimeError(f\"Download failed for {url}: {e}\") from e\n",
    "\n",
    "#         size = dst.stat().st_size\n",
    "#         logger.info(f\"Downloaded {dst} ({size} bytes)\")\n",
    "\n",
    "#         # sanity check: must start with PK\n",
    "#         with dst.open(\"rb\") as f:\n",
    "#             head = f.read(8)\n",
    "#         if not head.startswith(b\"PK\"):\n",
    "#             preview = dst.read_bytes()[:512].decode(errors=\"replace\")\n",
    "#             raise ValueError(f\"Downloaded file does not look like a ZIP. Preview:\\n{preview}\")\n",
    "\n",
    "#     def extract_zip_file(self):\n",
    "#         zip_path = Path(self.config.local_data_file).resolve()\n",
    "#         unzip_path = Path(self.config.unzip_dir).resolve()\n",
    "#         unzip_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         if not zip_path.exists():\n",
    "#             raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
    "#         if zip_path.stat().st_size == 0:\n",
    "#             raise ValueError(f\"Zip file is empty: {zip_path}\")\n",
    "\n",
    "#         # top-level magic check\n",
    "#         with zip_path.open(\"rb\") as f:\n",
    "#             head = f.read(4)\n",
    "#         if not head.startswith(b\"PK\"):\n",
    "#             preview = zip_path.read_bytes()[:512].decode(errors=\"replace\")\n",
    "#             raise RuntimeError(f\"Top-level file is not a ZIP. Preview:\\n{preview}\")\n",
    "\n",
    "#         # extract top-level zip\n",
    "#         try:\n",
    "#             with zipfile.ZipFile(str(zip_path), \"r\") as zip_ref:\n",
    "#                 logger.info(\"Top-level zip contents: %s\", zip_ref.namelist()[:30])\n",
    "#                 zip_ref.extractall(str(unzip_path))\n",
    "#         except (zipfile.BadZipFile, EOFError) as e:\n",
    "#             sample = zip_path.read_bytes()[:1024].decode(errors=\"replace\")\n",
    "#             raise RuntimeError(f\"Failed to extract top-level zip. Size={zip_path.stat().st_size}. Preview:\\n{sample}\") from e\n",
    "\n",
    "#         # If the archive contains an inner 'data.zip', extract it properly\n",
    "#         inner = unzip_path / \"data.zip\"\n",
    "#         if inner.exists():\n",
    "#             if inner.stat().st_size == 0:\n",
    "#                 raise RuntimeError(f\"Inner data.zip extracted as 0 bytes: {inner}\")\n",
    "#             # verify inner magic bytes\n",
    "#             with inner.open(\"rb\") as f:\n",
    "#                 if not f.read(4).startswith(b\"PK\"):\n",
    "#                     preview = inner.read_bytes()[:512].decode(errors=\"replace\")\n",
    "#                     raise RuntimeError(f\"Inner file is not a ZIP. Preview:\\n{preview}\")\n",
    "#             # extract inner zip into same unzip_path (or a subdir)\n",
    "#             try:\n",
    "#                 with zipfile.ZipFile(str(inner), \"r\") as zf:\n",
    "#                     logger.info(\"Inner zip contents: %s\", zf.namelist()[:30])\n",
    "#                     zf.extractall(str(unzip_path))\n",
    "#             except (zipfile.BadZipFile, EOFError) as e:\n",
    "#                 sample = inner.read_bytes()[:1024].decode(errors=\"replace\")\n",
    "#                 raise RuntimeError(f\"Failed to extract inner data.zip. Size={inner.stat().st_size}. Preview:\\n{sample}\") from e\n",
    "\n",
    "#         # diagnostic: list extracted files and sizes\n",
    "#         for f in sorted(unzip_path.rglob(\"*\"))[:200]:\n",
    "#             try:\n",
    "#                 logger.info(\"extracted: %s (%d)\", f.relative_to(unzip_path), f.stat().st_size)\n",
    "#             except Exception:\n",
    "#                 logger.info(\"extracted: %s (error)\", f)\n",
    "# # ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20 18:57:07,911: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-11-20 18:57:07,916: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-11-20 18:57:07,919: INFO: common: created directory at: artifacts]\n",
      "[2025-11-20 18:57:07,921: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "status: 200\n",
      "headers: {'Connection': 'keep-alive', 'Content-Length': '23627009', 'Cache-Control': 'max-age=300', 'Content-Security-Policy': \"default-src 'none'; style-src 'unsafe-inline'; sandbox\", 'Content-Type': 'application/zip', 'ETag': 'W/\"cf8e6fcd6e30e06d4b81d01e8e8873b531e7cdbc435d0a4cf4ba04fdeb8c461d\"', 'Strict-Transport-Security': 'max-age=31536000', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'deny', 'X-XSS-Protection': '1; mode=block', 'X-GitHub-Request-Id': '1991:952F1:1E9AE5:33B9A3:691F0FBD', 'Accept-Ranges': 'bytes', 'Date': 'Thu, 20 Nov 2025 13:27:09 GMT', 'Via': '1.1 varnish', 'X-Served-By': 'cache-maa10238-MAA', 'X-Cache': 'HIT', 'X-Cache-Hits': '0', 'X-Timer': 'S1763645229.491131,VS0,VE1', 'Vary': 'Authorization,Accept-Encoding', 'Access-Control-Allow-Origin': '*', 'Cross-Origin-Resource-Policy': 'cross-origin', 'X-Fastly-Request-ID': 'da2a6031ffa474e8cd0786063ce3d6e9cdf12193', 'Expires': 'Thu, 20 Nov 2025 13:32:09 GMT', 'Source-Age': '88'}\n",
      "written bytes: 23627009\n",
      "[2025-11-20 18:57:15,800: INFO: 3552099710: Zip contents: ['data.zip', 'samsum-test.csv', 'samsum-train.csv', 'samsum-validation.csv', 'samsum_dataset/', 'samsum_dataset/dataset_dict.json', 'samsum_dataset/test/', 'samsum_dataset/test/cache-be4f1205f3be26be.arrow', 'samsum_dataset/test/data-00000-of-00001.arrow', 'samsum_dataset/test/dataset_info.json']]\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "    data_ingestion.extract_zip_file()\n",
    "except Exception as e:\n",
    "    import traceback, sys\n",
    "    print(\"Exception type:\", type(e).__name__)\n",
    "    print(\"Exception message:\", str(e))\n",
    "    traceback.print_exc()         # full traceback for debugging\n",
    "    raise                         # preserve original traceback\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsumz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
